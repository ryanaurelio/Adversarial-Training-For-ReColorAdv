{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTERNAL LIBRARIES\n",
    "import numpy as np \n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mister_ed\n",
    "import recoloradv.mister_ed.loss_functions as lf \n",
    "import recoloradv.mister_ed.utils.pytorch_utils as utils\n",
    "import recoloradv.mister_ed.utils.image_utils as img_utils\n",
    "import recoloradv.mister_ed.cifar10.cifar_loader as cifar_loader\n",
    "import recoloradv.mister_ed.cifar10.cifar_resnets as cifar_resnets\n",
    "import recoloradv.mister_ed.adversarial_training as advtrain\n",
    "import recoloradv.mister_ed.utils.checkpoints as checkpoints\n",
    "import recoloradv.mister_ed.adversarial_perturbations as ap \n",
    "import recoloradv.mister_ed.adversarial_attacks as aa\n",
    "import recoloradv.mister_ed.spatial_transformers as st\n",
    "import recoloradv.mister_ed.config as config\n",
    "\n",
    "# ReColorAdv\n",
    "import recoloradv.perturbations as pt\n",
    "import recoloradv.color_transformers as ct\n",
    "import recoloradv.color_spaces as cs\n",
    "from recoloradv import norms\n",
    "from recoloradv.utils import load_pretrained_cifar10_model, get_attack_from_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HidePrint:\n",
    "    def __enter__(self):\n",
    "        self._temp_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._temp_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_folders(level):\n",
    "    if os.path.exists(f\"../datasets/adv_{level}\"):\n",
    "        shutil.rmtree(f\"../datasets/adv_{level}\")\n",
    "    for i in range(len(class_names)):\n",
    "        os.makedirs(f\"../datasets/adv_{level}/train/{i}\")\n",
    "        os.makedirs(f\"../datasets/adv_{level}/test/{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Normalize\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# CIFAR-10 Dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='../datasets/',\n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='../datasets/',\n",
    "    train=False,\n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                     stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Batch Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, normalizer = load_pretrained_cifar10_model('pretrained_models/normal.resnet32.pt')\n",
    "\n",
    "if utils.use_gpu():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This threat model defines the regularization parameters of the attack.\n",
    "recoloradv_threat = ap.ThreatModel(pt.ReColorAdv, {\n",
    "    'xform_class': ct.FullSpatial, \n",
    "    'cspace': cs.CIELUVColorSpace(), # controls the color space used\n",
    "    'lp_style': 'inf',\n",
    "    'lp_bound': [0.06, 0.06, 0.06],  # [epsilon_1, epsilon_2, epsilon_3]\n",
    "    'xform_params': {\n",
    "      'resolution_x': 16,            # R_1\n",
    "      'resolution_y': 32,            # R_2\n",
    "      'resolution_z': 32,            # R_3\n",
    "    },\n",
    "    'use_smooth_loss': True,\n",
    "})\n",
    "\n",
    "\n",
    "# Now, we define the main optimization term (the Carlini & Wagner f6 loss).\n",
    "adv_loss = lf.CWLossF6(model, normalizer)\n",
    "\n",
    "# We also need the smoothness loss.\n",
    "smooth_loss = lf.PerturbationNormLoss(lp=2)\n",
    "\n",
    "# We combine them with a RegularizedLoss object.\n",
    "attack_loss = lf.RegularizedLoss({'adv': adv_loss, 'smooth': smooth_loss}, \n",
    "                                 {'adv': 1.0,      'smooth': 0.05},   # lambda = 0.05\n",
    "                                 negate=True) # Need this true for PGD type attacks\n",
    "\n",
    "# PGD is used to optimize the above loss.\n",
    "pgd_attack_obj = aa.PGD(model, normalizer, recoloradv_threat, attack_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_images(images, labels):\n",
    "    if utils.use_gpu():\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    # We run the attack for 10 iterations at learning rate 0.01.\n",
    "    with HidePrint():\n",
    "        perturbation = pgd_attack_obj.attack(images, labels, num_iterations=10, signed=False, \n",
    "                                             optimizer=optim.Adam, optimizer_kwargs={'lr': 0.01},\n",
    "                                             verbose=True)\n",
    "\n",
    "    # Now, we can collect the successful adversarial examples and display them.\n",
    "    successful_advs, successful_origs, idxs = perturbation.my_collect_successful(model, normalizer)\n",
    "\n",
    "    ori = torch.stack([transform(img) for img in images])\n",
    "    adv = torch.stack([transform(img) for img in successful_advs])\n",
    "        \n",
    "    return torch.cat((ori, adv), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/500] Loss: 1.9659\n",
      "Epoch [1/10], Step [200/500] Loss: 1.9216\n",
      "Epoch [1/10], Step [300/500] Loss: 1.5656\n",
      "Epoch [1/10], Step [400/500] Loss: 1.5689\n",
      "Epoch [1/10], Step [500/500] Loss: 1.6555\n",
      "Epoch [2/10], Step [100/500] Loss: 1.5189\n",
      "Epoch [2/10], Step [200/500] Loss: 1.4986\n",
      "Epoch [2/10], Step [300/500] Loss: 1.2944\n",
      "Epoch [2/10], Step [400/500] Loss: 1.2572\n",
      "Epoch [2/10], Step [500/500] Loss: 1.3478\n",
      "Epoch [3/10], Step [100/500] Loss: 1.3000\n",
      "Epoch [3/10], Step [200/500] Loss: 1.3908\n",
      "Epoch [3/10], Step [300/500] Loss: 1.1363\n",
      "Epoch [3/10], Step [400/500] Loss: 1.1941\n",
      "Epoch [3/10], Step [500/500] Loss: 1.2677\n",
      "Epoch [4/10], Step [100/500] Loss: 1.1718\n",
      "Epoch [4/10], Step [200/500] Loss: 1.2766\n",
      "Epoch [4/10], Step [300/500] Loss: 0.9847\n",
      "Epoch [4/10], Step [400/500] Loss: 1.0264\n",
      "Epoch [4/10], Step [500/500] Loss: 1.1744\n",
      "Epoch [5/10], Step [100/500] Loss: 1.1082\n",
      "Epoch [5/10], Step [200/500] Loss: 1.2135\n",
      "Epoch [5/10], Step [300/500] Loss: 0.8726\n",
      "Epoch [5/10], Step [400/500] Loss: 0.9904\n",
      "Epoch [5/10], Step [500/500] Loss: 1.1250\n",
      "Epoch [6/10], Step [100/500] Loss: 1.0730\n",
      "Epoch [6/10], Step [200/500] Loss: 1.1270\n",
      "Epoch [6/10], Step [300/500] Loss: 0.8726\n",
      "Epoch [6/10], Step [400/500] Loss: 0.8730\n",
      "Epoch [6/10], Step [500/500] Loss: 1.0876\n",
      "Epoch [7/10], Step [100/500] Loss: 1.0363\n",
      "Epoch [7/10], Step [200/500] Loss: 0.9844\n",
      "Epoch [7/10], Step [300/500] Loss: 0.8141\n",
      "Epoch [7/10], Step [400/500] Loss: 0.8230\n",
      "Epoch [7/10], Step [500/500] Loss: 0.9926\n",
      "Epoch [8/10], Step [100/500] Loss: 0.9444\n",
      "Epoch [8/10], Step [200/500] Loss: 1.0111\n",
      "Epoch [8/10], Step [300/500] Loss: 0.8047\n",
      "Epoch [8/10], Step [400/500] Loss: 0.8393\n",
      "Epoch [8/10], Step [500/500] Loss: 0.8765\n",
      "Epoch [9/10], Step [100/500] Loss: 0.9765\n",
      "Epoch [9/10], Step [200/500] Loss: 0.9855\n",
      "Epoch [9/10], Step [300/500] Loss: 0.6794\n",
      "Epoch [9/10], Step [400/500] Loss: 0.7857\n",
      "Epoch [9/10], Step [500/500] Loss: 0.9077\n",
      "Epoch [10/10], Step [100/500] Loss: 0.9294\n",
      "Epoch [10/10], Step [200/500] Loss: 0.9009\n",
      "Epoch [10/10], Step [300/500] Loss: 0.6738\n",
      "Epoch [10/10], Step [400/500] Loss: 0.8015\n",
      "Epoch [10/10], Step [500/500] Loss: 0.8613\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = attack_images(images, labels)\n",
    "        labels = torch.cat((labels, labels), 0)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model, 'pretrained_models/resnet_def_10_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('pretrained_models/resnet_10_epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.Pad(4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root='../datasets/',\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset_adv = torchvision.datasets.ImageFolder(root='../datasets/adv_iter/test', transform=transform)\n",
    "\n",
    "# Data Loader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader_adv = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset_adv,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            prediction = torch.max(model(images), 1)[1].data.squeeze()\n",
    "            predictions += (prediction.tolist())\n",
    "            correct += (prediction == labels).sum().item()\n",
    "            total += len(labels)\n",
    "    \n",
    "    return correct / total, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7641"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_loader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5989"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(model, test_loader_adv)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import recoloradv.mister_ed.utils.image_utils as img_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_utils.show_images([successful_origs[:15], successful_advs[:15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_utils.show_images(examples[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    outputs = model(successful_origs)\n",
    "    _, predicted_ori = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted_ori == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([class_names[y] for y in labels[:15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    outputs = model(successful_advs)\n",
    "    _, predicted_adv = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted_adv == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_idx = predicted_ori == labels\n",
    "diff_idx = predicted_ori != predicted_adv\n",
    "success_atk_idx = correct_idx & diff_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "successful_ori = torch.stack([image for image, idx in zip(successful_origs, success_atk_idx) if idx])\n",
    "successful_adv = torch.stack([image for image, idx in zip(successful_advs, success_atk_idx) if idx])\n",
    "img_utils.show_images([successful_ori, successful_adv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(successful_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for label, idx in zip(predicted_ori, success_atk_idx) if idx]\n",
    "print([class_names[y] for y in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for label, idx in zip(predicted_adv, success_atk_idx) if idx]\n",
    "print([class_names[y] for y in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_adv = torch.stack([successful_advs[i] for i in false_idx])\n",
    "img_utils.show_images(successful_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_adv = [predicted_adv[i] for i in false_idx]\n",
    "print([class_names[y] for y in labels_adv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
